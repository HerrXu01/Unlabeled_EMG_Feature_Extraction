
Epoch [1/50], Train Loss: 100.6091, Val Loss: 101.6658
Epoch [2/50], Train Loss: 98.2956, Val Loss: 100.7828
Epoch [3/50], Train Loss: 96.8184, Val Loss: 100.1907
Epoch [4/50], Train Loss: 95.1029, Val Loss: 100.0250
Epoch [5/50], Train Loss: 93.4403, Val Loss: 99.8283
Epoch [6/50], Train Loss: 91.9018, Val Loss: 99.5926
Epoch [7/50], Train Loss: 90.3907, Val Loss: 100.7834
Epoch [8/50], Train Loss: 88.7604, Val Loss: 100.9630
Epoch [9/50], Train Loss: 87.3996, Val Loss: 100.9360
Epoch [10/50], Train Loss: 85.7363, Val Loss: 102.8768
Epoch [11/50], Train Loss: 84.4561, Val Loss: 102.4935
Epoch [12/50], Train Loss: 83.3722, Val Loss: 102.4997
Epoch [13/50], Train Loss: 82.0629, Val Loss: 103.3230
Epoch [14/50], Train Loss: 80.8814, Val Loss: 104.3359
Epoch [15/50], Train Loss: 79.6590, Val Loss: 105.4666
Traceback (most recent call last):
  File "E:\semester_7\thesis_code\main.py", line 28, in <module>
    main()
  File "E:\semester_7\thesis_code\main.py", line 25, in main
    registry.get_task_class(args.task)(config).train()
  File "E:\semester_7\thesis_code\trainer\base_trainer.py", line 141, in train
    optimizer.step()
  File "D:\anaconda\envs\potnet\lib\site-packages\torch\optim\optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "D:\anaconda\envs\potnet\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "D:\anaconda\envs\potnet\lib\site-packages\torch\optim\adamw.py", line 161, in step
    adamw(params_with_grad,
  File "D:\anaconda\envs\potnet\lib\site-packages\torch\optim\adamw.py", line 218, in adamw
    func(params,
  File "D:\anaconda\envs\potnet\lib\site-packages\torch\optim\adamw.py", line 309, in _single_tensor_adamw
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt